\label{sec:statistical-analysis}
At the end of the pipeline, we have at our disposal flagged C programs, which have been fuzzed and timed. Now we have to conclude whether or not these programs are variable time or not. This is a very complex task, as a lot of factors can affect this. Notably, the research conducted by \citeauthor{Abel19a} demonstrates the difficulty of this, as determining the performance of instructions is very microarchitecture dependant. Additionally, \cite{Almeida16}'s work suggests that static analysis on LLVM can be done, but again suffers from architecture-specific modeling. Thus, we determined that our method should not consider all the intricate details of architecture-specific implementations. Instead, a statistical approach was deemed more suitable.

One statistical approach is to apply significance tests between classes of fuzzing inputs. If we can detect a difference in time between different kinds of inputs, then we can conclude with good probability, that the program is not constant time. Ideally, for constant-time programs, we would expect a univariate Gaussian with low variation when we measure the running time of the program for all kinds of inputs. Likewise, ideally, for variable-time programs, we hope to see a mixture distribution composed of two (or more) Gaussians with different means. For instance, if we observe two means: $\mu_1 \neq \mu_2$, then we can conclude with good probability, that most likely a branching instruction on the input dictates whether or not the program will have a running time close to $\mu_1$ or $\mu_2$.

\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

% Solution to when the first gaussian is eq to second gaussian plotted in subfigure b)
\def\gausssolution{5.170045172}
\begin{figure}[H]
\captionsetup[subfigure]{justification=centering}
\begin{subfigure}[t]{0.50\textwidth}
\resizebox{\linewidth}{!}{
    \input{tikz/gauss-constant.tex}
}%
\caption{A constant time program yielding\\a univariate Gaussian with $\mu = 4$.}
\label{fig:univargauss}
\end{subfigure}
\begin{subfigure}[t]{0.50\textwidth}
\resizebox{\linewidth}{!}{
    \input{tikz/gauss-variable.tex}
}%
\caption{A variable time program, yielding a mixture\\distribution composed of two Gaussian distributions\\having means: $\mu_1 = 4, \mu_2 = 6$ with different variances}
\label{fig:mixdisgauss}
\end{subfigure}
\caption{An example of what we would expect in theory, when we fuzz with different input classes.}
\label{fig:fuzzclass-statistics-example}
\end{figure}
As seen above in Figure \ref{fig:univargauss}, we have an example of what we, in theory, would think the clock-cycles distribution for a constant-time program would look like. Likewise, Figure \ref{fig:mixdisgauss} shows what the distribution could look like for a variable-time program.

Wishing our measurements will yield such distributions, and then detecting timing leakage is, however, not as simple as described above. \citeauthor{Coron_2004} introduced significance test techniques in general leakage detection; including both timing and power consumption leakage attacks. In the paper, they argue that there is a correlation between measured time and external parameters \citep{Coron_2004}. As an illustrative example, we could use input class A for 10 minutes followed by 10 minutes of fuzzing with input class B while recording the corresponding timing outcomes. It is important to consider that the 10-minute duration of fuzzing with class A might have triggered certain system mechanisms such as a built-in thermal throttle for the CPU, resulting in a reduction of processing speed. Consequently, when evaluating the second input class B it may exhibit a distinct mean value due to the altered conditions induced by the previous measurements. There are a lot of external events to consider that might introduce noise to our data -- so the authors' suggested guideline is to alternate between classes when we measure. This essentially makes sure that the external noise is equally applied to both classes.

The previously mentioned tool in \ref{sec:fuzzing}, DudeCT, by \citeauthor{dudect} works in the above fashion. They, however, extended \citeauthor{Coron_2004}'s approach, by not only interleaving the input classes but by randomly choosing one. According to the authors, this should further reduce noise.

%%%%%%%%%%%%
% PR TODO:
% I believe, even though this is something in the fuzzer,
% that it makes sense to have it in this section, as it fits 
% beautifully with the theory presented, which finally leads to our methodology
%%%%%%%%%%%%
Our methodology extends the above, where we further try to reduce noise, by fuzzing multiple times and picking the minimum measured time. This is visualized in Figure \ref{fig:noisered} below. 
\begin{figure}[H]
    \centering
    \input{tikz/stat-fuzzing_method.tex}
    \caption{Noise reduction by picking the minimum measurement of multiple runs.}
    \label{fig:noisered}
\end{figure}
For example, if we in our random class sequence \{A, B, B, C\} do two iterations:
\begin{center}
    \{A: 2, B: 3, B: 5, C: 2\} \\
    \{A: 4, B: 3, B: 4, C: 3\}
\end{center}
where A: 2 corresponds to the program given input from input class A took 2 clock cycles to run, then the final saved measurements would be: \{A: 2, B: 3, B: 4, C: 2\}. Note that the random class sequence samples once and fixes the input for all iterations.

Now we apply a significance test to our measurements. There is a great consensus that Welch's t-test \cite{WELCH1947} works great for the task of leakage detection \cite{cryptoeprint:2015/536}.
The test tests the null hypothesis that two populations have equal means, which aligns exactly with our described objectives. The strengths of utilizing Welch's t-test are that it is robust to different variances as seen in figure \ref{fig:mixdisgauss}, and that its sampling complexity is low. According to a testing methodology for side channel resistance validation by \citeauthor{Goodwill2011ATM}, comparing only two of our input classes \texttt{FIXED} and \texttt{UNIFORM} is sufficient.
This is also what is used in DudeCT \cite{dudect} with a critical value of $t = 10$. 
This is a quite high threshold for the test, resulting in constant time programs being flagged as variable time with a very low probability (Type I errors).
However, the drawback is an increased Type II error, where variable time programs will not show up as variable time.

Our methodology takes a more conservative approach, where instead of fixing a critical value, we set $p = 0.05$. This significance level will in turn give us an expected Type I error of 5\%; however, the test will perform better when dealing with variable time programs.

Finally, the minimal measurements are plotted by generating TikZ with a corresponding program assembly, an indicator showing whether the null hypothesis was rejected, and a list of conditional jump instructions are highlighted. We also plot means for different input classes, aiding in identifying whether or not the program has timing leakages. Note that our plot trims outliers by removing the top 5 percentile, however, the t-test is applied to all measurements.

