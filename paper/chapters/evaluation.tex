In this section, we evaluate the quality of our results and the OptiFuzz tool.
Generally, we found that the problem of timing vulnerabilities in \texttt{gcc} is prevalent and, according to OptiFuzz, between 0.15\% and 12.63\% of random C programs are vulnerable to timing attacks depending on what optimizations are used when compiling. 
We argue that these results are predominantly representative of the actual real world problem in the following.

Our results have surfaced real problems in \texttt{gcc} since they have led us to pinpoint exact problematic optimizations as well as specific code patterns that are vulnerable to timing attacks (see Section \ref{sec:specific-optimizations} and \ref{sec:vulnerable-expressions} respectively).
However, some uncertainties are present throughout our pipeline.
We would like to address these uncertainties and discuss their impact on our results in the following subsections.

\subsection{Code Generation and Inspection}
Code generation is possibly the most uncertain part of our pipeline since the generated programs are not representative of real-world programs.
However, developers tend to write short expressions using arithmetic, bitwise and comparison operators to force their code to be constant-time in the real world, much like the programs generated by OptiFuzz. 
Additionally, we have found multiple examples of constant-time code snippets looking dangerously close to the vulnerable expressions we identified in Section \ref{sec:vulnerable-expressions} \citep{fact,what-you-c} \todo{find more sources}.

\todo{Limited, might not be representative, but still used in the real world (find examples). Inspection is exact, and it looks like we don't flag too many false positives.}

\subsection{Fuzzing}
\todo{Noise is a problem, but we do use a state-of-the-art approach.
From our results only 4.4\% programs are rejected by Welch's t-test as false positives.}

\subsection{Statistical Analysis}

\todo{From results, we can see that most programs with conditional branches are rejected by the t-test.
This means that our underapproximation by the inspection is okay.}

\todo{Internal testing shows around 4.4\% of programs will be rejected by Welch's t-test due to noise. This is perfect since $p=0.05$}