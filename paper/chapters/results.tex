We have run OptiFuzz to generate around 600,000 programs, which have been fuzzed and analyzed under different compilers and optimization flags.
In this section, we present our results.

We have divided this section into four subsections.
First, we compare \texttt{gcc} and \texttt{clang} generally in terms of the number of timing vulnerabilities introduced.
Second, we present our results for general optimizations flags such as \texttt{O2} and \texttt{Os}.
Third, we present our results regarding specific optimizations.
Finally, we present our results regarding what operations are most likely to cause timing vulnerabilities when optimized.
All experiments were carried out on an Intel Core i7-9750H running Manjaro Linux 22.1.2 with kernel version Linux 5.15.112-1. 
For compilers, \texttt{clang} version 15.0.7 and \texttt{gcc} version 12.2.1 were used.
All experiments were run on a single core and with \texttt{-20 niceness} to minimize interference from other processes.

\subsection{gcc vs. clang}
Surprisingly, we were unable to find any timing vulnerabilities introduced by \texttt{clang} under any optimization flags.
This is in contrast to \texttt{gcc}, where we saw timing vulnerabilities across various optimization flags.
Specifically, we compiled 50,000 random programs with \texttt{clang} using various optimization flags (\texttt{O0}, \texttt{O1}, \texttt{O2}, \texttt{O3} and \texttt{Os}) out of which none contained conditional branching instructions.

The reason for \texttt{clang} not introducing any timing vulnerabilities is the compiler's utilization of \texttt{cmovcc} instructions. 
\texttt{cmovcc} is a family of constant-time conditional move instructions \citep{cmov-is-constant-time} that was introduced by Intel in 1995 \citep{cmov-from-1995}. 

Even though our results show that \texttt{clang} does not introduce any timing vulnerabilities, our research shows that timing vulnerabilities were identified in fairly recent versions of \texttt{clang} \citep{fact,what-you-c}. 
Furthermore, we have identified an optimization step in the most recent version of the LLVM backend, which \texttt{clang} uses, that removes \texttt{cmovcc} instructions and substitutes them for conditional branches if an efficiency gain can be obtained with high confidence \citep{llvm-optimizing-away-cmov}.

Evidently, \texttt{clang} introduces conditional branches, and thus potentially timing vulnerabilities, much more conservatively than \texttt{gcc}. 
This is favorable in the context of language-based security avoiding timing vulnerabilities, but seemingly, \texttt{clang} is not perfect in preventing timing vulnerabilities even though our results were not able to confirm this. 
The lack of confirmed results might be ascribed to the simplicity of the generated programs or the sample size.

As no timing vulnerabilities were found using \texttt{clang}, all following subsections will present data obtained only through \texttt{gcc}.

\subsection{General Optimizations}
We ran the OptiFuzz pipeline with the optimization flags \texttt{O0}, \texttt{O1}, \texttt{O2}, \texttt{O3} and \texttt{Os}. 
For each optimization flag, we generated 100,000 programs, which were then fuzzed and analyzed. 
The ASTs of the generated programs were restricted to have a max depth of 5.
Each program was fuzzed with 10,000 different inputs.
The results are shown in Table \ref{tab:general-optimizations} and visualized results for select data points can be seen in Appendix \ref{appendix:general-optimizations-results}.

\begin{table}[H]
  \centering
  \begin{tabular}{l|lllll}
                                           & \textbf{\texttt{O0}} & \textbf{\texttt{O1}} & \textbf{\texttt{O2}} & \textbf{\texttt{O3}} & \textbf{\texttt{Os}} \\ \hline
  \# of flagged programs                   & 19758                                 & 231                                   & 1005                                  & 999                                   & 366                                   \\
  \% of flagged programs                   & 19.76\%                               & 0.23\%                                & 1.00\%                                & 1.00\%                                & 0.37\%                                \\
  \# of programs with rejected $H_0$       & 12627                                 & 147                                   & 624                                   & 640                                   & 199                                   \\
  \% of rejected programs (out of total)   & 12.63\%                               & 0.15\%                                & 0.62\%                                & 0.64\%                                & 0.20\%                                \\
  \% of rejected programs (out of flagged) & 63.91\%                               & 63.64\%                               & 62.09\%                               & 64.06\%                               & 54.37\%                              
  \end{tabular}
  \caption{Results for the optimization flags \texttt{O0}, \texttt{O1}, \texttt{O2}, \texttt{O3} and \texttt{Os} using \texttt{gcc}. 
  The results are based on 100,000 generated programs for each optimization flag. 
  Each program is generated from an AST of max depth 5.
  The first and second rows show the number and percentage of programs that contained conditional branching instructions after compilation (potential timing leak).
  The third, fourth and fifth rows show the number and percentage of programs that resulted in $H_0$ getting rejected in Welch's t-test (definite timing leak).}
  \label{tab:general-optimizations}
\end{table}
Our results show that the problem is very prevalent in \texttt{gcc} with \texttt{O0}, surprisingly, being the worst offender where over a tenth of the randomly generated programs were rejected by Welch's t-test. 
The fact that \texttt{O0} introduces timing vulnerabilities with such a high probability is concerning since \texttt{O0} has most configurable optimizations turned off \citep{O0-has-most-opts-turned-off}.
This points to the fact that it is not easy for the developer to mitigate problematic optimizations in \texttt{gcc}.

Our experiment revealed some limitations to our approach to finding timing vulnerabilities. 
False positives, i.e. constant-time code that was rejected by Welch's t-test, were present in the dataset from the \texttt{O0} experiment. 
This is likely due to noise in the measurements. 
Experimentally, we found that Welch's t-test will reject around 4.4\% of programs that are constant-time due to noise.
Since such a high number of programs compiled with \texttt{O0} contained branching instructions, this noise was enough to cause a significant amount of false positives compared to the amount of programs generated for the experiment.
It is also possible that seemingly constant-time programs are not constant-time due to hardware optimizations, like branch prediction or speculative execution, which are not taken into account in our model and are out-of-scope for this paper.
However, our results also show that definite timing leaks are present in the \texttt{O0} dataset as seen in Figure \ref{fig:general-optimizations-O0-true-positive}.

\begin{figure}[H]
  \centering
     \begin{subfigure}[b]{0.55\textwidth}
      \begin{tikzpicture}[>=latex]
        \begin{axis}[
            axis x line=center,
            axis y line=center,
            name=ax,
            scaled y ticks=base 10:-3,
            ytick scale label code/.code={},
            yticklabel={\pgfmathprintnumber{\tick} k},
            xlabel={CPU Clocks},
            ylabel={Frequency},
            x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
            y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south,yshift=4mm},
            area style,
            ymin=0,
            xmin=19,
            xmax=38,
            ymax=1213
            ]
            \addplot+ [ybar interval,mark=no,color=firstCol,fill=firstCol,fill opacity=0.2] table {
                20 1050
21 21
            };
\addplot+ [ybar interval,mark=no,color=firstCol,fill=firstCol,fill opacity=0.2] table {
                20 1145
21 16
            };
\addplot+ [ybar interval,mark=no,color=firstCol,fill=firstCol,fill opacity=0.2] table {
                20 765
21 13
22 308
23 1
24 0
25 1
26 0
27 0
28 0
29 0
30 0
31 0
32 0
33 0
34 0
35 0
36 0
37 1
            };
\addplot+ [ybar interval,mark=no,color=firstCol,fill=firstCol,fill opacity=0.2] table {
                20 708
21 70
22 10
23 5
24 6
25 1
26 2
27 5
28 299
29 0
30 1
31 0
32 0
33 0
34 0
35 0
36 0
37 2
            };
\addplot+ [ybar interval,mark=no,color=firstCol,fill=firstCol,fill opacity=0.2] table {
                20 671
21 60
22 14
23 3
24 5
25 3
26 4
27 8
28 282
29 16
30 3
31 1
32 0
33 1
34 0
35 0
36 0
37 1
            };
\addplot+ [ybar interval,mark=no,color=firstCol,fill=firstCol,fill opacity=0.2] table {
                20 34
21 294
22 127
23 19
24 0
25 13
26 7
27 5
28 5
29 0
30 4
31 605
32 19
33 0
34 0
35 3
36 1
37 2
            };
\addplot+ [ybar interval,mark=no,color=firstCol,fill=firstCol,fill opacity=0.2] table {
                20 692
21 61
22 9
23 7
24 5
25 4
26 1
27 7
28 287
29 18
30 1
31 0
32 0
33 2
34 0
35 0
36 0
37 2
            };
\addplot+ [ybar interval,mark=no,color=firstCol,fill=firstCol,fill opacity=0.2] table {
                20 1155
21 18
            };
\addplot+ [ybar interval,mark=no,color=firstCol,fill=firstCol,fill opacity=0.2] table {
                20 350
21 367
22 72
23 0
24 8
25 7
26 0
27 9
28 1
29 0
30 1
31 6
32 0
33 261
34 6
35 0
36 1
37 2
            };
                                \draw[color=black, line width=0.2mm, dashed] 
                (axis cs:20, -60.650000000000006) -- (axis cs:20, 1213);
                \draw[color=black, line width=0.2mm, dashed] 
                (axis cs:20, -60.650000000000006) -- (axis cs:20, 1213);
                \draw[color=black, line width=0.2mm, dashed] 
                (axis cs:21, -60.650000000000006) -- (axis cs:21, 1213);
                \draw[color=black, line width=0.2mm, dashed] 
                (axis cs:22, -60.650000000000006) -- (axis cs:22, 1213);
                \draw[color=black, line width=0.2mm, dashed] 
                (axis cs:23, -60.650000000000006) -- (axis cs:23, 1213);
                \draw[color=black, line width=0.2mm, dashed] 
                (axis cs:27, -60.650000000000006) -- (axis cs:27, 1213);
                \draw[color=black, line width=0.2mm, dashed] 
                (axis cs:22, -60.650000000000006) -- (axis cs:22, 1213);
                \draw[color=black, line width=0.2mm, dashed] 
                (axis cs:20, -60.650000000000006) -- (axis cs:20, 1213);
                \draw[color=black, line width=0.2mm, dashed] 
                (axis cs:24, -60.650000000000006) -- (axis cs:24, 1213);
        \end{axis} 
        \node[below=15mm of ax] (1) {
            $\begin{aligned}
                \texttt{equal}_\mu: & \,20\\
                \texttt{fixed}_\mu: & \,20\\
                \texttt{max64}_\mu: & \,21
            \end{aligned}$
        };
        \node[left=4mm of 1] (2) {
            $\begin{aligned}
                \texttt{small}_\mu: & \,22\\
                \texttt{uniform}_\mu: & \,23\\
                \texttt{xlty}_\mu: & \,27
            \end{aligned}$
        };
        \node[right=4mm of 1] (3) {
            $\begin{aligned}
                \texttt{xzero}_\mu: & \,22\\
                \texttt{yltx}_\mu: & \,20\\
                \texttt{yzero}_\mu: & \,24
            \end{aligned}$
        };
        \node[fit=(1)(2)(3),draw]{};
        \end{tikzpicture}%
      \caption{Fuzzing results visualized. The means of each fuzz class are shown as vertical lines.}
    \end{subfigure}
    \hspace{1cm}
    \begin{subfigure}[b]{0.35\textwidth}
      \begin{lstlisting}[style=defstyle,language={[x86masm]Assembler},basicstyle=\small\ttfamily,breaklines=true]
...
  pushq	%rbp
  movq	%rsp, %rbp
  movq	%rdi, -8(%rbp)
  movq	%rsi, -16(%rbp)
  movq	-8(%rbp), %rax
  cmpq	-16(%rbp), %rax
  jge	.L2
  movq	$-2, %rdx
  movabsq	$1153072017525752320, %rax
  movl	%edx, %ecx
  sarq	%cl, %rax
  cmpq	$-1, %rax
  jl	.L3
.L2:
  movl	$1, %eax
  jmp	.L5
.L3:
  movl	$0, %eax
.L5:
  popq	%rbp
  ret
...\end{lstlisting} 
       \caption{Fuzzed assembly code.}
  \end{subfigure}
  \caption{An example of a true positive, i.e. a variable time program that was rejected by Welch's t-test, from the \texttt{O0} dataset. The source code of the program is shown in Appendix \ref{appendix:general-optimizations-results}.}
  \label{fig:general-optimizations-O0-true-positive}
\end{figure}

Other findings from the other datasets also deserve mentioning.
The \texttt{O1}, \texttt{O2}, \texttt{O3} and \texttt{Os} datasets had several false negatives, i.e. programs that were not rejected by Welch's t-test, but were variable time.
Concretely, we saw examples where the assembled code branched on a \textit{very} specific scenario, e.g. if $x = 8129346172934691$, which is near-impossible to detect by fuzzing since the probability of hitting that specific value is very low.
We did not find any false positives in any other dataset than \texttt{O0}.
This is because the vast majority of programs in the \texttt{O1}, \texttt{O2}, \texttt{O3} and \texttt{Os} datasets were variable time.
This points to the fact that more aggressive optimizations with conditional branching are more likely to produce detectably variable-time code.
The reason that Welch's t-test did not detect vulnerabilities in all programs in the datasets is likely due to the above-mentioned false negatives, but also because the t-test has some inherent limitations as mentioned in Section \ref{sec:statistical-analysis}.
Concrete examples of true and false positives and negatives can be found in Appendix \ref{appendix:general-optimizations-results}.

Finally, we want to address the fact that the general optimizations datasets were generated with programs that had ASTs of max depth 5.
This was done to reduce the length of the programs to make them more manageable for analysis.
However, we also generated datasets with ASTs of max depth 12 and found that the results were similar in the \texttt{O0} and \texttt{O1} datasets, while the \texttt{O2}, \texttt{O3} and \texttt{Os} datasets had significantly more programs with conditional branching -- around 5\%, 5\% and 2\% respectively.
We deem the choice of max depth 5 to be a good compromise between the length of the programs and the number of conditional branches, and our results underline the problem we are trying to address.

\subsection{Specific Optimizations}
\todo{Single out specific optimizations}

\subsection{Vulnerable operations}
\todo{Try to identify what operations are vulnerable}
