\label{sec:evaluation}
In this section, we evaluate the quality of our results and the OptiFuzz tool.
Generally, we found that the problem of timing vulnerabilities in \texttt{gcc} is prevalent and, according to OptiFuzz, between 0.15\% and 12.63\% of random C programs are vulnerable to timing attacks depending on what optimizations are used when compiling. 
We argue that these results are predominantly representative of the actual real-world problem in the following.

Our results have surfaced real problems in \texttt{gcc} since they have led us to pinpoint exact problematic optimizations as well as specific code patterns that are vulnerable to timing attacks (see Section \ref{sec:specific-optimizations} and \ref{sec:vulnerable-expressions} respectively).
However, some uncertainties are present throughout our pipeline.
We would like to address these uncertainties and discuss their impact on our results in the following subsections.

\subsection{Code Generation and Inspection}
Code generation might seem the most uncertain part of our pipeline since the generated programs are not representative of real-world programs.
However, developers tend to write short expressions using arithmetic, bitwise and comparison operators to force their code to be constant-time in the real world, much like the programs generated by OptiFuzz. 
We have found multiple examples of constant-time code snippets looking dangerously close to the vulnerable expressions we identified in Section \ref{sec:vulnerable-expressions} \citep{fact,what-you-c,verifying-constant-time-llvm}.
Furthermore, we have identified the use of this paradigm in the wild, for example in the x86 AES implementation in the OpenSSL library \citep{openssl}.
Hence quantifying the problem of timing vulnerabilities in the generated programs is indicative of the real-world problem.

The inspection of the generated programs is exact since it finds all problematic instructions in the generated assembly code as argued in Section \ref{sec:inspection}.
Furthermore, our results show that between 54.37\% and 64.06\% of the programs that are flagged by inspection are eventually rejected by the statistical analysis (see Section \ref{sec:statistical-analysis}), confirming they are variable-time. 
This means that, even though inspection serves as an overapproximation of the problem, it does not cause too big of an overhead in the OptiFuzz pipeline.

\subsection{Fuzzing and Statistical Analysis}
Our results show that some noise is present in our fuzzing data.
As discussed in Section \ref{sec:general-optimizations}, this caused false positives in our results.
Concretely, we found that around 4.4\% of programs are rejected by Welch's t-test as false positives due to noise when fuzzing.
This percentage was found by fuzzing a large number of definite constant-time programs and observing how many of them were rejected by Welch's t-test after fuzzing.
This is not alarming since it is well within the chosen significance level of $p = 0.05$ for Welch's t-test.

Furthermore, from manual analysis, we found that there are quite a lot of false negatives present in the data.
As argued in Section \ref{sec:general-optimizations}, this is due to a large number of programs branching on very specific inputs.
This means that the programs are not rejected by Welch's t-test since the branch is not taken often enough to cause a significant difference in the execution time.
This is a problem since the programs are still vulnerable to timing attacks.
OptiFuzz is still a great tool in this particular case, as our analysis and visualization pipeline aids in the manual
discovery of timing leaks.

Additionally, we only applied statistical analysis between the input classes \texttt{fixed} and \texttt{uniform}.
This means that we have missed some vulnerable programs that branch on other input classes.
An example of this can be seen in Appendix \ref{appendix:general-optimizations-results} (Os False Negative).
This observation indicates an opportunity for future work where the statistical analysis could be improved to handle these cases and perhaps more input classes could be considered.
But, as seen in the results in Section \ref{sec:general-optimizations}, the statistical analysis is still able to reject the majority of vulnerable programs.

In total, the pipeline does not seem to have any major flaws that would cause the results to be unrepresentative of the real-world problem.
