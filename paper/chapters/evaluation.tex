In this section, we evaluate the quality of our results and the OptiFuzz tool.
Generally, we found that the problem of timing vulnerabilities in \texttt{gcc} is prevalent and, according to OptiFuzz, between 0.15\% and 12.63\% of random C programs are vulnerable to timing attacks depending on what optimizations are used when compiling. 
We argue that these results are predominantly representative of the actual real world problem in the following.

Our results have surfaced real problems in \texttt{gcc} since they have led us to pinpoint exact problematic optimizations as well as specific code patterns that are vulnerable to timing attacks (see Section \ref{sec:specific-optimizations} and \ref{sec:vulnerable-expressions} respectively).
However, some uncertainties are present throughout our pipeline.
We would like to address these uncertainties and discuss their impact on our results in the following subsections.

\subsection{Code Generation and Inspection}
Code generation might seem the most uncertain part of our pipeline since the generated programs are not representative of real-world programs.
However, developers tend to write short expressions using arithmetic, bitwise and comparison operators to force their code to be constant-time in the real world, much like the programs generated by OptiFuzz. 
We have found multiple examples of constant-time code snippets looking dangerously close to the vulnerable expressions we identified in Section \ref{sec:vulnerable-expressions} \citep{fact,what-you-c}.
Furthermore, we have identified the use of this paradigm in the wild, for example in x86 AES implementation of the OpenSSL library \citep{openssl}.
Hence quantifying the problem of timing vulnerabilities in the generated programs is indicative of the real-world problem.

The inspection of the generated programs is exact since it finds all problematic instructions in the generated assembly code as argued in Section \ref{sec:inspection}.
Furthermore, our results show that between 54.37\% and 64.06\% of the programs that are flagged by inspection are eventually rejected by the statistical analysis (see Section \ref{sec:statistical-analysis}). 
This means that, even though inspection serves as an overapproximation of the problem, it does not cause too big of an overhead in the OptiFuzz pipeline.

\subsection{Fuzzing}
\todo{Noise is a problem, but we do use a state-of-the-art approach.
From our results only 4.4\% programs are rejected by Welch's t-test as false positives.}

\subsection{Statistical Analysis}

\todo{From results, we can see that most programs with conditional branches are rejected by the t-test.
This means that our underapproximation by the inspection is okay.}

\todo{Internal testing shows around 4.4\% of programs will be rejected by Welch's t-test due to noise. This is perfect since $p=0.05$}